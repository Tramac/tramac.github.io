<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|0.7:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-loading-bar.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"tramac.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Visual Representation  Efficient Self-supervised Vision Transformers for Representation Learning-2021&lt;Paper&gt;  Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations-2">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper-reading List">
<meta property="og:url" content="http://tramac.github.io/2018/11/18/paper-reading-list/index.html">
<meta property="og:site_name" content="Tramac&#39;s Space">
<meta property="og:description" content="Visual Representation  Efficient Self-supervised Vision Transformers for Representation Learning-2021&lt;Paper&gt;  Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations-2">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2018-11-18T07:41:18.000Z">
<meta property="article:modified_time" content="2021-09-14T09:01:29.726Z">
<meta property="article:author" content="Tramac">
<meta property="article:tag" content="Paper">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://tramac.github.io/2018/11/18/paper-reading-list/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Paper-reading List | Tramac's Space</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Tramac's Space" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
    <a href="https:///github.com/tramac" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tramac's Space</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Keep trying</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://tramac.github.io/2018/11/18/paper-reading-list/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tramac">
      <meta itemprop="description" content="Tramac写字的地方">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tramac's Space">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Paper-reading List
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-11-18 15:41:18" itemprop="dateCreated datePublished" datetime="2018-11-18T15:41:18+08:00">2018-11-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-09-14 17:01:29" itemprop="dateModified" datetime="2021-09-14T17:01:29+08:00">2021-09-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-reading-List/" itemprop="url" rel="index"><span itemprop="name">Paper-reading List</span></a>
                </span>
            </span>

          
            <span id="/2018/11/18/paper-reading-list/" class="post-meta-item leancloud_visitors" data-flag-title="Paper-reading List" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/11/18/paper-reading-list/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/11/18/paper-reading-list/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Visual-Representation"><a href="#Visual-Representation" class="headerlink" title="Visual Representation"></a>Visual Representation</h2><ul>
<li><input disabled="" type="checkbox"> Efficient Self-supervised Vision Transformers for Representation Learning-2021&lt;<a href="https://arxiv.org/pdf/2106.09785.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> Revisiting Contrastive Methods for Unsupervised Learning of Visual Representations-2021&lt;<a href="https://arxiv.org/abs/2106.05967" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/wvangansbeke/Revisiting-Contrastive-SSL" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
</ul>
<h2 id="Video-Understanding"><a href="#Video-Understanding" class="headerlink" title="Video Understanding"></a>Video Understanding</h2><ul>
<li><input disabled="" type="checkbox"> ViViT: A Video Vision Transformer-Arxiv2021(&lt;<a href="https://arxiv.org/pdf/2103.15691.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/rishikksh20/ViViT-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;)</li>
<li><input disabled="" type="checkbox"> Video Swin Transformer-Arxiv2021&lt;<a href="https://arxiv.org/pdf/2106.13230.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/SwinTransformer/Video-Swin-Transformer" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE-ICLR2021&lt;<a href="https://openreview.net/pdf?id=YicbFdNTTy" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/lucidrains/vit-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> A Survey of Transformers-Arxiv2021&lt;<a href="https://arxiv.org/pdf/2106.04554.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features-CBMI2021&lt;<a href="https://arxiv.org/pdf/2106.00358.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision-ICML2021&lt;<a href="https://arxiv.org/pdf/2102.03334.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/dandelin/ViLT" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers-CVPR2021&lt;<a href="https://arxiv.org/pdf/2103.16553.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> TEACHTEXT: CrossModal Generalized Distillation for Text-Video Retrieval-2021&lt;<a href="https://arxiv.org/pdf/2104.08271.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/albanie/collaborative-experts" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> Cross-Modal Retrieval Augmentation for Multi-Modal Classification-2021&lt;<a href="https://arxiv.org/pdf/2104.08108.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> Continual learning in cross-modal retrieval-CVPR2021&lt;<a href="https://arxiv.org/pdf/2104.06806.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning-CVPR2021&lt;[Paper]&gt;&lt;<a href="https://github.com/researchmm/soho" target="_blank" rel="noopener">Code</a>&gt;</li>
<li><input disabled="" type="checkbox"> Visual Semantic Role Labeling for Video Understanding-CVPR2021&lt;<a href="https://arxiv.org/pdf/2104.00990.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/TheShadow29/VidSitu" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> Perceiver: General Perception with Iterative Attention-2021&lt;<a href="https://arxiv.org/pdf/2103.03206.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning-CVPR2021&lt;<a href="https://arxiv.org/pdf/2103.13061.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/amzn/image-to-recipe-transformers" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> Hyperbolic Visual Embedding Learning for Zero-Shot Recognition-CVPR2020&lt;<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Hyperbolic_Visual_Embedding_Learning_for_Zero-Shot_Recognition_CVPR_2020_paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ShaoTengLiu/Hyperbolic_ZSL" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> Retrieve Fast, Rerank Smart:Cooperative and Joint Approaches for Improved Cross-Modal Retrieval-2021&lt;<a href="https://arxiv.org/pdf/2103.11920.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/UKPLab/MMT-Retrieval" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> What is Multimodality?&lt;<a href="https://arxiv.org/pdf/2103.06304.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Multi-modal Transformer for Video Retrieval-ECCV2020&lt;<a href="https://arxiv.org/pdf/2007.10639.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/gabeur/mmt" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> Support-set Bottlenecks for Video-text Representation Learning-ICLR2021&lt;<a href="https://arxiv.org/pdf/2010.02824.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Dual Encoding for Video Retrieval by Text-TPAMI2021&lt;<a href="https://arxiv.org/pdf/2009.05381.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/danieljf24/hybrid_space" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling-CVPR2021&lt;<a href="https://arxiv.org/pdf/2102.06183.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/jayleicn/ClipBERT" target="_blank" rel="noopener">Code</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> VL-BERT: Pre-training of Generic Visual-Linguistic Representations-ICLR2020&lt;<a href="https://arxiv.org/pdf/1908.08530.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/jackroos/VL-BERT" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> Transformer is All You Need:Multimodal Multitask Learning with a Unified Transformer-2021&lt;<a href="https://arxiv.org/pdf/2102.10772.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://mmf.sh/" target="_blank" rel="noopener">Code</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning-NeurIPS2020&lt;<a href="https://proceedings.neurips.cc/paper/2020/file/ff0abbcc0227c9124a804b084d161a2d-Paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/gingsi/coot-videotext" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning-CVPR2020&lt;<a href="https://arxiv.org/pdf/2003.00392.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/cshizhe/hgr_v2t" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> LXMERT: Learning Cross-Modality Encoder Representations from Transformers-EMNLP2019&lt;<a href="https://arxiv.org/pdf/1908.07490.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/airsplay/lxmert" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> VisualBERT: A Simple and Performant Baseline for Vision and Language-2019&lt;<a href="https://arxiv.org/pdf/1908.03557.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/uclanlp/visualbert" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> Video SemNet: Memory-Augmented Video Semantic Network-NIPS2017&lt;<a href="https://arxiv.org/pdf/2011.10909.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Self-Supervised Video Representation Learning by Pace Prediction-ECCV2020&lt;<a href="https://arxiv.org/pdf/2008.05861.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/laura-wang/video-pace" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> SalSum: Saliency-based Video Summarization using Generative Adversarial Networks-2020&lt;<a href="https://arxiv.org/pdf/2011.10432.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> Self-Supervised Temporal-Discriminative Representation Learning for Video Action Recognition-2020&lt;<a href="https://arxiv.org/pdf/2008.02129.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/FingerRec/Self-Supervised-Temporal-Discriminative-Representation-Learning-for-Video-Action-Recognition" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://zhuanlan.zhihu.com/p/176774543" target="_blank" rel="noopener">Zhihu</a>&gt;</li>
<li><input disabled="" type="checkbox"> Classification of Important Segments in Educational Videos using Multimodal Features-CIKM2020&lt;<a href="https://arxiv.org/pdf/2010.13626.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/VideoAnalysis/EDUVSUM" target="_blank" rel="noopener">Code-Keras</a>&gt;</li>
<li><input disabled="" type="checkbox"> Attentive and Adversarial Learning for Video Summarization-WACV2019&lt;<a href="https://tsujuifu.github.io/pubs/wacv19_vsum-ptr-gan.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/tsujuifu/pytorch_vsum-ptr-gan" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Digital Video Summarization Techniques: A Survey-2020&lt;<a href="https://www.ijert.org/digital-video-summarization-techniques-a-survey" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Emerging Trends of Multimodal Research in Vision and Language-2020&lt;<a href="https://arxiv.org/pdf/2010.09522.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> Exploring global diverse attention via pairwise temporal relation for video summarization-2020&lt;<a href="https://arxiv.org/pdf/2009.10942.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Multi-modal Dense Video Captioning-CVPR Workshops 2020&lt;<a href="https://arxiv.org/pdf/2003.07758.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/v-iashin/MDVC" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://v-iashin.github.io/mdvc" target="_blank" rel="noopener">Project</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Accuracy and Performance Comparison of Video Action Recognition Approaches-HPEC2020&lt;<a href="https://arxiv.org/pdf/2008.09037.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> What Makes Training Multi-Modal Classification Networks Hard?-CVPR2020&lt;<a href="https://arxiv.org/pdf/1905.12681.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> [<strong>DMASum</strong>] Query Twice: Dual Mixture Attention Meta Learning for Video Summarization-ACM2020&lt;<a href="https://arxiv.org/pdf/2008.08360.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input disabled="" type="checkbox"> [<strong>CHAN</strong>] Convolutional Hierarchical Attention Network for Query-Focused Video Summarization&lt;<a href="https://arxiv.org/pdf/2002.03740.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ckczzj/AAAI2020" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> [<strong>ILS-SUMM</strong>] ILS-SUMM: Iterated Local Search for Unsupervised Video Summarization-ICPR2020&lt;<a href="https://arxiv.org/pdf/1912.03650.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/YairShemer/ILS-SUMM" target="_blank" rel="noopener">Code</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward-AAAI2018&lt;<a href="https://arxiv.org/abs/1801.00054" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/KaiyangZhou/pytorch-vsumm-reinforce" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> [<strong>SUM-GAN</strong>] Unsupervised video summarization with adversarial lstm networks-CVPR2017&lt;<a href="http://web.engr.oregonstate.edu/~sinisa/research/publications/cvpr17_summarization.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/j-min/Adversarial_Video_Summary" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Enhancing Video Summarization via Vision-Language Embedding-CVPR2017&lt;<a href="https://slazebni.cs.illinois.edu/publications/cvpr17_summarization.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Query-adaptive Video Summarization via Quality-aware Relevance Estimation-ICCV2017&lt;<a href="https://arxiv.org/pdf/1705.00581.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/arunbalajeev/query-video-summary" target="_blank" rel="noopener">Code-Theano</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Temporal Tessellation: A Unified Approach for Video Analysis-ICCV2017&lt;<a href="https://arxiv.org/pdf/1612.06950.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/dot27/temporal-tessellation" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Video Summarization with Long Short-term Memory-ECCV2016&lt;<a href="https://arxiv.org/pdf/1605.08110.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/kezhang-cs/Video-Summarization-with-LSTM" target="_blank" rel="noopener">Code-Theano</a>&gt;&lt;<a href="https://github.com/MagedMilad/Video-Summarization-with-Long-Short-term-Memory" target="_blank" rel="noopener">Code-Keras</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> Video Summarization using Deep Semantic Features-ACCV2016&lt;<a href="https://arxiv.org/pdf/1609.08758v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/mayu-ot/vsum_dsf" target="_blank" rel="noopener">Code-Chainer</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> [<strong>SA-LSTM</strong>] Describing Videos by Exploiting Temporal Structure-ICCV2015&lt;<a href="https://arxiv.org/pdf/1502.08029.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yaoli/arctic-capgen-vid" target="_blank" rel="noopener">Code-Theano</a>&gt;&lt;<a href="https://github.com/hobincar/SA-LSTM" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> [<strong>3D-ResNet</strong>] Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?-CVPR2018&lt;<a href="https://arxiv.org/pdf/1711.09577.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/kenshohara/3D-ResNets-PyTorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> [<strong>Hidden Two-Stream</strong>] Hidden Two-Stream Convolutional Networks for Action Recognition-ACCV2018&lt;<a href="https://arxiv.org/pdf/1704.00389.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/bryanyzhu/two-stream-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> [FlowNet2] FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks&lt;[Paper(<a href="https://arxiv.org/pdf/1612.01925.pdf)]&gt;" target="_blank" rel="noopener">https://arxiv.org/pdf/1612.01925.pdf)]&gt;</a>&lt;<a href="https://github.com/NVIDIA/flownet2-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> [<strong>TSN</strong>] Temporal Segment Networks Towards Good Practices for Deep Action Recognition-ECCV2016&lt;<a href="https://arxiv.org/pdf/1608.00859.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yjxiong/temporal-segment-networks" target="_blank" rel="noopener">Code-Caffe</a>&gt;&lt;<a href="https://github.com/yjxiong/tsn-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input disabled="" type="checkbox"> Towards Good Practices for Very Deep Two-Stream ConvNets&lt;<a href="https://arxiv.org/pdf/1507.02159.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yjxiong/caffe/tree/action_recog" target="_blank" rel="noopener">Code-Caffe</a>&gt;</li>
<li><input disabled="" type="checkbox"> [<strong>Two-Stream</strong>] Two-Stream Convolutional Networks for Action Recognition in Videos-NIPS2014&lt;<a href="https://arxiv.org/pdf/1406.2199.pdf" target="_blank" rel="noopener">Paper</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> [<strong>C3D</strong>] Learning Spatiotemporal Features with 3D Convolutional Networks-ICCV2015&lt;<a href="https://arxiv.org/pdf/1412.0767.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/facebookarchive/C3D" target="_blank" rel="noopener">Code-Caffe</a>&gt;&lt;<a href="https://github.com/hx173149/C3D-tensorflow" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;&lt;<a href="https://github.com/jfzhang95/pytorch-video-recognition" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
<li><input checked="" disabled="" type="checkbox"> [<strong>NetVLAD</strong>] NetVLAD: CNN architecture for weakly supervised place recognition-CVPR2016&lt;<a href="https://arxiv.org/pdf/1511.07247.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/Relja/netvlad" target="_blank" rel="noopener">Code-Matlab</a>&gt;&lt;<a href="https://github.com/lyakaap/NetVLAD-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</li>
</ul>
<a id="more"></a>

<h2 id="Semantic-Segmentation"><a href="#Semantic-Segmentation" class="headerlink" title="Semantic Segmentation"></a>Semantic Segmentation</h2><p><a href="https://github.com/Tramac/Awesome-semantic-segmentation-pytorch" target="_blank" rel="noopener">GitHub</a></p>
<p>1.[<strong>AdaptSegNet</strong>] Learning to Adapt Structured Output Space for Semantic Segmentation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1802.10349.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/wasidennis/AdaptSegNet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>2.[<strong>DAM/DCM</strong>] Unsupervised Cross-Modality Domain Adaptation of ConvNets for Biomedical Image Segmentations with Adversarial Loss-IJCAI2018&lt;<a href="https://arxiv.org/pdf/1804.10916.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>3.[<strong>FCAN</strong>] Fully Convolutional Adaptation Networks for Semantic Segmentation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1804.08286.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>4.[<strong>DenseASPP</strong>] DenseASPP for Semantic Segmentation in Street Scenes-CVPR2018&lt;<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_DenseASPP_for_Semantic_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/DeepMotionAIResearch/DenseASPP" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>5.Dense Decoder Shortcut Connections for Single-Pass Semantic Segmentation-CVPR2018&lt;<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Bilinski_Dense_Decoder_Shortcut_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>6.[<strong>AotofocusLayer</strong>] Autofocus Layer for Semantic Segmentation-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1805.08403.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yaq007/Autofocus-Layer" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>7.[<strong>PDV-Net</strong>] Automatic Segmentation of Pulmonary Lobes Using a Progressive Dense V-Network-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00889-5_32.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>8.[<strong>RR-SegSE</strong>] Adaptive feature recombination and recalibration for semantic segmentation: application to brain tumor segmentation in MRI-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1806.02318.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/sergiormpereira/rr_segse" target="_blank" rel="noopener">Code</a>&gt;<br>9.[<strong>HD-Net</strong>] Fine-Grained Segmentation Using Hierarchical Dilated Neural Networks-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00937-3_56.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>10.[<strong>U-JAPA-Net</strong>] 3D U-JAPA-Net: Mixture of Convolutional Networks for Abdominal Multi-organ CT Segmentation-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00937-3_49.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>11.[<strong>CompNet</strong>] CompNet: Complementary Segmentation Network for Brain MRI Extraction-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1804.00521.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/raun1/MICCAI2018---Complementary_Segmentation_Network-Raw-Code" target="_blank" rel="noopener">Code-Keras</a>&gt;<br>12.Deep Learning-Based Boundary Detection for Model-Based Segmentation with Application to MR Prostate Segmentation-MICCAI2018&lt;<a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-00937-3_59.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>13.[<strong>RS-Net</strong>] RS-Net: Regression-Segmentation 3D CNN for Synthesis of Full Resolution Missing Brain MRI in the Presence of Tumours-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1807.10972v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/RagMeh11/RS-Net" target="_blank" rel="noopener">Code</a>&gt;<br>14.CT-Realistic Lung Nodule Simulation from 3D Conditional Generative Adversarial Networks for Robust Lung Segmentation-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1806.04051.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>15.[<strong>CB-GANs</strong>] Learning Data Augmentation for Brain Tumor Segmentation with Coarse-to-Fine Generative Adversarial Networks-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1805.11291.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>16.[<strong>FSENet</strong>] Focus, Segment and Erase: An Efficient Network for Multi-Label Brain Tumor Segmentation-ECCV2018&lt;<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Xuan_Chen_Focus_Segment_and_ECCV_2018_paper.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/LaviniaChen/Segment-and-Erase-Network" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>17.[<strong>DeepLabv3+</strong>] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation-ECCV2018&lt;<a href="https://arxiv.org/pdf/1802.02611v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/tensorflow/models/tree/master/research/deeplab" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>18.[<strong>ExFuse</strong>] ExFuse: Enhancing Feature Fusion for Semantic Segmentation-ECCV2018&lt;<a href="https://arxiv.org/pdf/1804.03821.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>19.[<strong>ESPNet</strong>] ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation-ECCV2018&lt;<a href="https://arxiv.org/pdf/1803.06815v2.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/sacmehta/ESPNet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>20.[<strong>EncNet</strong>] Context Encoding for Semantic Segmentation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1803.08904v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/zhanghang1989/PyTorch-Encoding" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>21.[<strong>PSPNet</strong>] Pyramid Scene Parsing Network-CVPR2017&lt;<a href="https://arxiv.org/pdf/1612.01105.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/hszhao/PSPNet" target="_blank" rel="noopener">Code-Caffe</a>&gt;<br>22.[<strong>DANet</strong>] Dual Attention Network for Scene Segmentation-CVPR2019&lt;<a href="https://arxiv.org/pdf/1809.02983.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/junfu1115/DANet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>23.[<strong>BiSeNet</strong>] BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation-ECCV-2018&lt;<a href="https://arxiv.org/pdf/1808.00897.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ycszen/TorchSeg" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>24.[<strong>Fast-SCNN</strong>] Fast-SCNN: Fast Semantic Segmentation Network-2019&lt;<a href="https://arxiv.org/pdf/1902.04502.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/Tramac/Fast-SCNN-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>25.[<strong>ICNet</strong>] ICNet for Real-Time Semantic Segmentation on High-Resolution Images-ECCV2018&lt;<a href="https://arxiv.org/pdf/1704.08545.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/Tramac/awesome-semantic-segmentation-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://github.com/hszhao/ICNet" target="_blank" rel="noopener">Code-Caffe</a>&gt;<br>26.[<strong>DUNet</strong>] Decoders Matter for Semantic Segmentation: Data-Dependent Decoding Enables Flexible Feature Aggregation-CVPR2019&lt;<a href="https://arxiv.org/abs/1903.02120.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/Tramac/awesome-semantic-segmentation-pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>27.[<strong>ENet</strong>] ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation-2016&lt;<a href="https://arxiv.org/abs/1606.02147.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/TimoSaemann/ENet" target="_blank" rel="noopener">Code-Caffe</a>&gt;&lt;<a href="https://github.com/davidtvs/PyTorch-ENet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://github.com/kwotsin/TensorFlow-ENet" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>28.[<strong>CCNet</strong>] CCNet: Criss-Cross Attention for Semantic Segmentation-2018&lt;<a href="https://arxiv.org/abs/1811.11721v1.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/speedinghzl/CCNet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>29.[<strong>OCNet</strong>] OCNet: Object Context Network for Scene Parsing-2018&lt;<a href="https://arxiv.org/abs/1809.00916.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/PkuRainBow/OCNet.pytorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>30.[<strong>HRNet</strong>] High-Resolution Representations for Labeling Pixels and Regions-2019&lt;<a href="https://arxiv.org/pdf/1904.04514.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/HRNet/HRNet-Semantic-Segmentation" target="_blank" rel="noopener">Code-PyTorch</a>&gt;</p>
<h2 id="Panoptic-Segmentation"><a href="#Panoptic-Segmentation" class="headerlink" title="Panoptic Segmentation"></a>Panoptic Segmentation</h2><p>1.[<strong>Panoptic FPN</strong>] Panoptic Feature Pyramid Networks-Arxiv2019&lt;<a href="https://arxiv.org/pdf/1901.02446.pdf" target="_blank" rel="noopener">Paper</a>&gt;</p>
<h2 id="Super-Resolution"><a href="#Super-Resolution" class="headerlink" title="Super-Resolution"></a>Super-Resolution</h2><p>1.[<strong>mDCSRN</strong>] Efficient and Accurate MRI Super-Resolution using a Generative Adversarial Network and 3D Multi-Level Densely Connected Network-MICCAI2018&lt;<a href="https://arxiv.org/pdf/1803.01417.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>2.[<strong>RDN</strong>] Residual Dense Network for Image Super-Resolution-CVPR2018&lt;<a href="https://arxiv.org/pdf/1802.08797.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/yulunzhang/RDN" target="_blank" rel="noopener">Code-Torch</a>&gt;&lt;<a href="https://github.com/thstkdgus35/EDSR-PyTorch" target="_blank" rel="noopener">Code-PyTorch</a>&gt;&lt;<a href="https://github.com/hengchuan/RDN-TensorFlow" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;</p>
<h2 id="Networks-Architecture"><a href="#Networks-Architecture" class="headerlink" title="Networks Architecture"></a>Networks Architecture</h2><p>1.[<strong>DLA</strong>] Deep Layer Aggregation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1707.06484.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ucbdrive/dla" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>2.[<strong>DualSkipNet</strong>] Dual Skipping Networks-CVPR2018&lt;<a href="https://arxiv.org/pdf/1710.10386.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>3.[<strong>SkipNet</strong>] SkipNet: Learning Dynamic Routing in Convolutional Networks-ECCV2018&lt;<a href="https://arxiv.org/pdf/1711.09485.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/ucbdrive/skipnet" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>4.[<strong>DRN</strong>] Dilated Residual Networks-CVPR2017&lt;<a href="https://arxiv.org/pdf/1705.09914.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/fyu/drn" target="_blank" rel="noopener">Code-PyTorch</a>&gt;<br>5.[<strong>CapsNet</strong>] Dynamic Routing Between Capsules-NIPS2017&lt;<a href="https://arxiv.org/pdf/1710.09829.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/naturomics/CapsNet-Tensorflow" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>6.[<strong>BlockQNN</strong>] Practical Block-wise Neural Network Architecture Generation-CVPR2018&lt;<a href="https://arxiv.org/pdf/1708.05552.pdf" target="_blank" rel="noopener">Paper</a>&gt;<br>7.[<strong>MobileNetV2</strong>] MobileNetV2: Inverted Residuals and Linear Bottlenecks-CVPR2018&lt;<a href="https://128.84.21.199/pdf/1801.04381.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank" rel="noopener">Code-Tensorflow</a>&gt;<br>8.[<strong>Non-Local</strong>] Non-local Neural Networks-CVPR2018&lt;<a href="https://arxiv.org/pdf/1711.07971.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/facebookresearch/video-nonlocal-net" target="_blank" rel="noopener">Code</a>&gt;</p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>1.[<strong>FocalLoss</strong>]Focal Loss for Dense Object Detection-ICCV2017&lt;<a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank" rel="noopener">Paper</a>&gt;&lt;<a href="https://github.com/facebookresearch/Detectron" target="_blank" rel="noopener">Code-Caffe2</a>&gt;</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Paper/" rel="tag"># Paper</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/11/06/PyTorch%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%8AFinetunning/" rel="prev" title="PyTorch参数初始化及Finetunning">
      <i class="fa fa-chevron-left"></i> PyTorch参数初始化及Finetunning
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/12/16/Finetuning%20with%20Tensorflow/" rel="next" title="Finetuning with Tensorflow">
      Finetuning with Tensorflow <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Visual-Representation"><span class="nav-number">1.</span> <span class="nav-text">Visual Representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Video-Understanding"><span class="nav-number">2.</span> <span class="nav-text">Video Understanding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Semantic-Segmentation"><span class="nav-number">3.</span> <span class="nav-text">Semantic Segmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Panoptic-Segmentation"><span class="nav-number">4.</span> <span class="nav-text">Panoptic Segmentation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Super-Resolution"><span class="nav-number">5.</span> <span class="nav-text">Super-Resolution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Networks-Architecture"><span class="nav-number">6.</span> <span class="nav-text">Networks Architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loss-Function"><span class="nav-number">7.</span> <span class="nav-text">Loss Function</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Tramac"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Tramac</p>
  <div class="site-description" itemprop="description">Tramac写字的地方</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Tramac" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Tramac" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/qia-ka-ka-23" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;qia-ka-ka-23" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tramac@yeah.net" title="E-Mail → mailto:tramac@yeah.net" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tramac</span>
</div>

        






<script>
  (function() {
    function leancloudSelector(url) {
      url = encodeURI(url);
      return document.getElementById(url).querySelector('.leancloud-visitors-count');
    }

    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.id);
      var title = visitors.dataset.flagTitle;

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url })))
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            leancloudSelector(url).innerText = counter.time + 1;
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              });
          } else {
              Counter('post', '/classes/Counter', { title, url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.id);
      });

      Counter('get', '/classes/Counter?where=' + encodeURIComponent(JSON.stringify({ url: { '$in': entries } })))
        .then(response => response.json())
        .then(({ results }) => {
          for (let url of entries) {
            let target = results.find(item => item.url === url);
            leancloudSelector(url).innerText = target ? target.time : 0;
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }

    let { app_id, app_key, server_url } = {"enable":true,"app_id":"EwM0rzNJJvrzOMGM9QtRQabg-gzGzoHsz","app_key":"Cgarq2zhnKAf5zzYVctu26G2","server_url":null,"security":false};
    function fetchData(api_server) {
      var Counter = (method, url, data) => {
        return fetch(`${api_server}/1.1${url}`, {
          method,
          headers: {
            'X-LC-Id'     : app_id,
            'X-LC-Key'    : app_key,
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    }

    let api_server = app_id.slice(-9) !== '-MdYXbMMI' ? server_url : `https://${app_id.slice(0, 8).toLowerCase()}.api.lncldglobal.com`;

    if (api_server) {
      fetchData(api_server);
    } else {
      fetch('https://app-router.leancloud.cn/2/route?appId=' + app_id)
        .then(response => response.json())
        .then(({ api_server }) => {
          fetchData('https://' + api_server);
        });
    }
  })();
</script>


      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'EwM0rzNJJvrzOMGM9QtRQabg-gzGzoHsz',
      appKey     : 'Cgarq2zhnKAf5zzYVctu26G2',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
